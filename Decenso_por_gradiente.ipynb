{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decenso por gradiente.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tozanni/Data_Science_Notebooks/blob/main/Decenso_por_gradiente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descenso por gradiente paso a paso\n",
        "\n",
        "Este cuaderno ha sido traducido y adaptado de la siguiente referencia.\n",
        "\n",
        "[Alan Reiner - Introduction to Custom Gradient Descent in Tensorflow 2.0](https://github.com/etotheipi/toptal_tensorflow_blog_post/blob/dev/simple_height_vs_weight/tf_grad_desc_intro.ipynb)"
      ],
      "metadata": {
        "id": "xJcanHFBz6MM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyID9FFOCr9e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import display, Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leer el archivo"
      ],
      "metadata": {
        "id": "qv_pzeOyzBbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/tozanni/Data_Science_Notebooks/main/height_v_weight.csv')\n",
        "df = df.drop(labels=' Gender', axis=1)\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "7Nzm4thCC392"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_mean_sq_error(df, slope, intercept):\n",
        "    \"\"\"\n",
        "    Calcula el error cuadrático medio (MSE) de la línea como predictor de pesos\n",
        "    \"\"\"\n",
        "    hs, ws = df['Height'].values, df['Weight'].values\n",
        "    diffs = (slope * hs + intercept) - ws\n",
        "    mse = np.mean(diffs**2) \n",
        "    return mse\n",
        "    \n",
        "    \n",
        "def draw_scatter(df, slope=None, intercept=None, ax=None):\n",
        "    if ax is None:\n",
        "        _,ax = plt.subplots(figsize=(4,4))\n",
        "        \n",
        "    sns.scatterplot(df['Height'], df['Weight'], ax=ax, label='Sample Points')\n",
        "    ax.set_xlabel('Height (inches)')\n",
        "    ax.set_ylabel('Weight (lbs)')\n",
        "    ax.set_title('Male Height vs. Weight Sample')\n",
        "    \n",
        "    # For this exercise we're going to hardcode various parameters for simplicity\n",
        "    h0,h1 = 60, 78\n",
        "    w0,w1 = 130, 260\n",
        "    ax.set_xlim(h0, h1)\n",
        "    ax.set_ylim(w0, w1)\n",
        "    ax.set_aspect((h1-h0)/(w1-w0))\n",
        "    \n",
        "    if slope is not None and intercept is not None:\n",
        "        pred_w0, pred_w1 = (slope*h0 + intercept, slope*h1 + intercept)\n",
        "        ax.plot([h0, h1], [pred_w0, pred_w1], 'r-.', label='Fitted Line')\n",
        "        mse = calc_mean_sq_error(df, slope, intercept)\n",
        "        eqn_str = f'w = {slope:.2f} * h + {intercept:.1f}'\n",
        "        ax.set_title(f'Loss: MSE={mse:.1f}\\n{eqn_str}')\n",
        "        \n",
        "    ax.legend(loc='upper left')\n",
        "    return ax"
      ],
      "metadata": {
        "id": "FNgr2CMLEeH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(10,5))\n",
        "draw_scatter(df, slope=4,  intercept = -120, ax=axs[0])\n",
        "draw_scatter(df, slope=2,  intercept =   70, ax=axs[1])\n",
        "draw_scatter(df, slope=3,  intercept =  -30, ax=axs[2])\n",
        "plt.tight_layout(2)"
      ],
      "metadata": {
        "id": "h3hC1jb4Eguk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontremos la solución analítica usando sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(df['Height'].values.reshape([-1, 1]), df['Weight'].values)\n",
        "\n",
        "# Veamos los parámetros óptimos del modelo y su pérdida MSE\n",
        "true_slope, true_intercept = lin_reg.coef_[0], lin_reg.intercept_\n",
        "min_mse = calc_mean_sq_error(df, true_slope, true_intercept)\n",
        "\n",
        "_ = draw_scatter(df, slope=true_slope, intercept=true_intercept)"
      ],
      "metadata": {
        "id": "5Y6FSQM5Ek51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando Tensorflow y Gradient Descent\n",
        "\n",
        "Para un problema tan simple con una solución analítica, no necesitamos usar el descenso de gradiente. Pero el objetivo aquí es presentar la función de diferenciación automática de Tensorflow en un problema simple para tener una idea de su mecánica sin complejidad. También veremos que se trata de soluciones razonables, incluso si en realidad no son óptimas."
      ],
      "metadata": {
        "id": "qri5V2oQ0E58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_calc_mse_verbose(heights, weights, slope, intercept):\n",
        "    \"\"\"\n",
        "    Este método es básicamente idéntico a calc_mse() de antes, pero\n",
        "     necesita usar solo operaciones definidas en tensores y métodos tf.*.\n",
        "    \"\"\"\n",
        "    predictions = tf.add(tf.multiply(slope, heights), intercept)\n",
        "    errors = tf.subtract(predictions, weights)\n",
        "    mse = tf.reduce_mean(tf.square(errors))\n",
        "    return mse\n",
        "    \n",
        "def tf_calc_mse_clean(heights, weights, slope, intercept):\n",
        "    \"\"\"\n",
        "    Tensor algebra can be implemented with standard operators, simplifying\n",
        "    the expressions.  This method is identical to the one above.\n",
        "    \"\"\"\n",
        "    predictions = slope * heights + intercept\n",
        "    errors = predictions - weights\n",
        "    mse = tf.reduce_mean(errors**2)\n",
        "    return mse"
      ],
      "metadata": {
        "id": "_R6pBbnIEqbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_linreg_progress(df, slope, intercept, mse_hist, min_mse, file_out=None, n_iter=25, fig=None, axs=None):\n",
        "    if fig is None or axs is None:\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(8,4))\n",
        "        \n",
        "    draw_scatter(df, slope, intercept, ax=axs[0])\n",
        "    \n",
        "    # Draw loss-chart.  Hardcode a few more parameters for simplicity\n",
        "    hmin, hmax = 0, n_iter\n",
        "    wmin, wmax = 0, 2000\n",
        "    axs[1].plot(range(len(mse_hist)), mse_hist, 'b-', marker='o', label='Computed Loss')\n",
        "    axs[1].plot([hmin, hmax], [min_mse, min_mse], 'g-.', label='Minimum Possible Loss')\n",
        "    axs[1].set_xlim(hmin, hmax)\n",
        "    axs[1].set_ylim(wmin, wmax)\n",
        "    axs[1].set_xlabel('Iteration')\n",
        "    axs[1].set_ylabel('Loss (MSE)')\n",
        "    axs[1].legend(loc='upper right')\n",
        "    axs[1].set_aspect(hmax/float(wmax))\n",
        "    \n",
        "    # Update the axis titles\n",
        "    mse = calc_mean_sq_error(df, slope, intercept) if len(mse_hist)==0 else mse_hist[-1]\n",
        "    axs[0].set_title('Male Heights & Weights')\n",
        "    eqn_str = f'w = {slope:.2f} * h + {intercept:.1f}'\n",
        "    axs[1].set_title(f'Loss: MSE={mse:.1f}\\n{eqn_str}')\n",
        "    #plt.tight_layout(3.0)\n",
        "    \n",
        "    if file_out:\n",
        "        fig.savefig(file_out)\n",
        "        \n",
        "    return fig, axs\n",
        "    \n",
        "_ = draw_linreg_progress(df, true_slope, true_intercept, [], min_mse)\n",
        "\n"
      ],
      "metadata": {
        "id": "j8UHIOtaEuOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gradient_descent(df, init_slope, init_icept, n_iter=25, learning_rate=2e-5, dir_name=None):\n",
        "    \"\"\"\n",
        "     Proporciona una estimación inicial de la pendiente y la intersección, el descenso de la pendiente se ajustará\n",
        "     y proporcionará una solución cercana a la óptima\n",
        "    \"\"\"\n",
        "    \n",
        "    Hs, Ws = df['Height'].values, df['Weight'].values\n",
        "    \n",
        "    # Las variables que seran parte de los cálculos de gradiente deben de convertirse\n",
        "    # mediante la funcion tf.Variable de TensorFlow\n",
        "\n",
        "    tf_slope = tf.Variable(init_slope, dtype='float32') \n",
        "    tf_icept = tf.Variable(init_icept, dtype='float32') \n",
        "    \n",
        "    # Acumular el historial de pérdida de cada época\n",
        "    loss_hist = []\n",
        "    shutil.rmtree(dir_name, ignore_errors=True)\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 2, figsize=(9,4))\n",
        "    \n",
        "    for i in range(n_iter):\n",
        "        \n",
        "        # tf.GradientTape() es el objeto que lleva registro de todos los cálculos \n",
        "        # de tensores diferenciables en el bloque de código\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Indicar a gradientTape que queremos llevar registro de la pendiente\n",
        "            # y del intercepto\n",
        "\n",
        "            tape.watch((tf_slope, tf_icept))\n",
        "\n",
        "            # Calcular la pérdida \n",
        "            predictions = tf_slope * Hs + tf_icept\n",
        "            errors = predictions - Ws\n",
        "            loss = tf.reduce_mean(errors**2)\n",
        "\n",
        "        #########################################################################\n",
        "        # Magia!  Obtener la derivada de la pérdida con respecto a los parámetros\n",
        "        dloss_dparams = tape.gradient(loss, [tf_slope, tf_icept])\n",
        "        #########################################################################\n",
        "\n",
        "        # Dado que no normalizamos los datos y la pendiente tiene diferente magnitud\n",
        "        # que el intercepto, debemos ajustar los gradientes del intercepto por una \n",
        "        # constante razonable (en este caso 1000)\n",
        "        tf_slope = tf_slope - learning_rate * dloss_dparams[0]\n",
        "        tf_icept = tf_icept - learning_rate * dloss_dparams[1] * 1000.0\n",
        "            \n",
        "        # Registrar y graficar el valor de la pérdida\n",
        "        loss_hist.append(loss)\n",
        "        if dir_name:\n",
        "            os.makedirs(dir_name, exist_ok=True)\n",
        "            fout = os.path.join(dir_name, f'img_{i:03d}.png')\n",
        "            axs[0].clear()\n",
        "            axs[1].clear()\n",
        "            draw_linreg_progress(df, tf_slope, tf_icept, loss_hist, min_mse, n_iter=n_iter, file_out=fout, fig=fig, axs=axs)\n",
        "        \n",
        "    return tf_slope.numpy(), tf_icept.numpy(), loss_hist[-1]\n",
        "        \n"
      ],
      "metadata": {
        "id": "cda5hl3UEzZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Veamos qué tan bien funciona!\n",
        "\n",
        "Resulta que la superficie de pérdida es un dificil de lo que cabría esperar, especialmente porque no normalizamos nuestros datos de antemano. Si bien esto debería funcionar teóricamente, terminamos necesitando algunas tasas de aprendizaje por parámetro súper dinámicas para que funcione a partir de valores iniciales elegidos arbitrariamente.\n",
        "\n",
        "Para evitar complicar esto en exceso, simplemente lo ejecutaremos a partir de una suposición inicial razonable de los parámetros y dejaremos que ajuste la respuesta por nosotros. En la mayoría de los problemas de descenso de gradiente."
      ],
      "metadata": {
        "id": "mxIZydTi0wDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slope1, int1, loss1 = run_gradient_descent(df, true_slope, -180, dir_name='imgs_descent_1')"
      ],
      "metadata": {
        "id": "rlJCVQ8vE3JR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}